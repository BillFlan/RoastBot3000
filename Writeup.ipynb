{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roast Bot\n",
    "#### Asher Farr\n",
    "#### CSCI 4622 - Machine Learning\n",
    "https://github.com/BillFlan/RoastBot3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Introduction:\n",
    "The idea of this project was to try and build a neural network which, when given an image of a person, would output a comedic roast of the person. I am really interested in NLP and if a computer can learn to understand comedy. (Spoiler warning: If I'm doing it, they can't)\n",
    "\n",
    "\n",
    "Basically, the goal was to produce a comedic caption for a given image. \n",
    "\n",
    "In order to do this, I built a dataset from the reddit community r/RoastMe. The set contained images and the associated top 5 comments on the post, which by the rules of the community are all roasts.\n",
    "\n",
    "I attempted to impliment this using an encoder-decoder structure, using a pretrained CNN as the encoder and a transformer as the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data:\n",
    "As stated in the introduction, the data for this project was scraped using reddit's api. All data was gathered from reddit.com/r/RoastMe. A typical post/roast combo looks like this:\n",
    "\n",
    "**Image**: <img src=\"https://i.redd.it/qcrbkirlgm541.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "**Roast**: \"Did it really take you 8 pictures to realize that the camera was never the problem?\"\n",
    "\n",
    "The process of image gathering was basically looping over a large number of posts, downloading the images and saving associated comments to a JSON file.\n",
    "\n",
    "I actually went way too overboard with this and I gathered around 26GB of images and about 34MB of comments, totalling around 400,000 individual comments and 70,000 images, which was a much larger dataset than I was feasably able to process and train on in the amount of time for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing:\n",
    "Since this data was gathered from a pubilic source on the internet, it was obviously not clean. \n",
    "\n",
    "* For my initial pass, actually gahtering the data, I filtered out images that were labelled NSFW (not safe for work) by the poster on reddit. \n",
    "* On the next pass I removed any corrupted image files and their corrosponding comments from the dataset which reduced its size by about a gigabyte.\n",
    "* The third pass was about making the school appropriate and to remove any deleted posts. Basically I applied a profanity filter library to the comments and removed the offensive ones, so the network would not have cursing in its set of words. Then I removed any deleted posts because they are also not useful data and would make the network think that '[deleted]' is a funny comment\n",
    "\n",
    "After all this, the data was still way too big to do anything with on my computer or on google colab which I used for most of my processing. The CNN I ended up using required images be 299x299 pixels, so I did another pass to resize all of the images which reduced the total file size to 650Mb. This is obvioulsy still not a small dataset, but it was now much less unwieldy compared to the 26Gb behemoth I had before.\n",
    "\n",
    "Overall this was a super time consuming process and just downloading all the data took about 7-8 hours, but I think I got some very interesting data from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA:\n",
    "Honestly for this dataset, I wasn't really sure how to explore it very well. I found that had an average word count of 50 words, with a maximum of 430.\n",
    "\n",
    "From here I decided it would be okay to further reduce the amount of data used to comments with 62 words or fewer in order to match the output dimensionality of the CNN. This was done in order to more easily integrate the output of the CNN into the transfomer's multi-head attention which we will discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model:\n",
    "This model was based on a paper called \"Captioning Transformer with Stacked Attention Modules\" and uses a CNN, specifically InceptionV3 to encode information about the image, then sends it to a Transformer model (ref: \"Attention is all you need\") in order to generate captions for the images.\n",
    "\n",
    "It looks like this: <img src=\"./transformer.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "In order to impliment the model, I used Google Brain's Trax repository because they are the main group developing new transformer technology. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "Let's just say this thing did not work super well. I'm not sure if there was an error in my implementation, or if I just didn't train it on enough data, but this transformer does not understand english very well.\n",
    "\n",
    "It is clearly getting some encoding from the enviroment, but it really doesn't get how to make jokes.\n",
    "\n",
    "For example: I inputed this image of my friends\n",
    "<img src=\"./adamLiam.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "And the resulting roast was \"You look like an arm guy\"\n",
    "\n",
    "So the model clearly is getting something from the CNN, but it doesn't have enough information encoded in it.\n",
    "\n",
    "Another Example I thought was funny (but because it was bad):\n",
    "\n",
    "**Input**:\n",
    "<img src=\"./boss-hog.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "**Output**:\n",
    "\"Nice Hat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion:\n",
    "With this particular project, I think I was really reaching beyond my abilities. If I had *a lot* more time and GPUs I think I could have produced somthing with a more impressive grasp of comedy.\n",
    "\n",
    "A couple of approaches to try in the future:\n",
    "* Using a beam search to widen the amount of possiblities searched through before writing the sentence\n",
    "* Using a much larger subset of the data\n",
    "* Running the CNN through some sort of pretrained Encoder like BERT\n",
    "* Using a pretrained Decoder with a stronger grasp of English like GPT-2\n",
    "\n",
    "Overall, I think this is a very interesting problem in Machine Learning because comedy is generally poorly understood, so it is a very difficult NLP problem to tackle. I think in the future, I will continue to mess with this model to hopefully produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
